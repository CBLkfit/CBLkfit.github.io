<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>RoBERTa Usage | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="git clone https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;fairseq.git cd fairseq pip install --editable .&#x2F; For MacOS: CFLAGS&#x3D;&quot;-stdlib&#x3D;libc++&quot; pip install --editable .&#x2F; 接下来可以参考https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;fairseq&#x2F;b">
<meta property="og:type" content="article">
<meta property="og:title" content="RoBERTa Usage">
<meta property="og:url" content="http://example.com/2020/12/28/RoBERTa%20Usage/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="git clone https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;fairseq.git cd fairseq pip install --editable .&#x2F; For MacOS: CFLAGS&#x3D;&quot;-stdlib&#x3D;libc++&quot; pip install --editable .&#x2F; 接下来可以参考https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;fairseq&#x2F;b">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-12-28T01:50:06.000Z">
<meta property="article:modified_time" content="2020-12-28T08:47:26.905Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-RoBERTa Usage" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/12/28/RoBERTa%20Usage/" class="article-date">
  <time class="dt-published" datetime="2020-12-28T01:50:06.000Z" itemprop="datePublished">2020-12-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      RoBERTa Usage
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><code>git clone https://github.com/pytorch/fairseq.git</code></p>
<p><code>cd fairseq</code></p>
<p><code>pip install --editable ./</code></p>
<p>For MacOS:</p>
<p><code>CFLAGS=&quot;-stdlib=libc++&quot; pip install --editable ./</code></p>
<p>接下来可以参考<a target="_blank" rel="noopener" href="https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.pretraining.md">https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.pretraining.md</a> </p>
<p>下载数据集<em>WikiText-103</em>：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https:<span class="comment">//s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip</span></span><br><span class="line">unzip wikitext-<span class="number">103</span>-raw-v1.zip</span><br></pre></td></tr></table></figure>
<p>将数据集按照GPT-2 BPE进行编码：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p gpt2_bpe</span><br><span class="line">wget -O gpt2_bpe/encoder.json https:<span class="comment">//dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json</span></span><br><span class="line">wget -O gpt2_bpe/vocab.bpe https:<span class="comment">//dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe</span></span><br><span class="line"><span class="keyword">for</span> SPLIT <span class="keyword">in</span> train valid test; <span class="keyword">do</span> \</span><br><span class="line">    python -m examples.roberta.multiprocessing_bpe_encoder \</span><br><span class="line">        --encoder-json gpt2_bpe/encoder.json \</span><br><span class="line">        --vocab-bpe gpt2_bpe/vocab.bpe \</span><br><span class="line">        --inputs wikitext-<span class="number">103</span>-raw/wiki.$&#123;SPLIT&#125;.raw \</span><br><span class="line">        --outputs wikitext-<span class="number">103</span>-raw/wiki.$&#123;SPLIT&#125;.bpe \</span><br><span class="line">        --keep-empty \</span><br><span class="line">        --workers <span class="number">60</span>; \</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">wget -O gpt2_bpe/dict.txt https:<span class="comment">//dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt</span></span><br><span class="line">fairseq-preprocess \</span><br><span class="line">    --only-source \</span><br><span class="line">    --srcdict gpt2_bpe/dict.txt \</span><br><span class="line">    --trainpref wikitext-<span class="number">103</span>-raw/wiki.train.bpe \</span><br><span class="line">    --validpref wikitext-<span class="number">103</span>-raw/wiki.valid.bpe \</span><br><span class="line">    --testpref wikitext-<span class="number">103</span>-raw/wiki.test.bpe \</span><br><span class="line">    --destdir data-bin/wikitext-<span class="number">103</span> \</span><br><span class="line">    --workers <span class="number">60</span></span><br></pre></td></tr></table></figure>
<p>得到<em>data-bin/wikitext-103</em> 文件夹，文件夹下是预处理好的数据集。</p>
<p>接下来可以开始训练啦，需要用到GPU资源。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">TOTAL_UPDATES=125000    # Total number of training steps</span><br><span class="line">WARMUP_UPDATES=10000    # Warmup the learning rate over this many updates</span><br><span class="line">PEAK_LR=0.0005          # Peak learning rate, adjust as needed</span><br><span class="line">TOKENS_PER_SAMPLE=512   # Max sequence length</span><br><span class="line">MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)</span><br><span class="line">MAX_SENTENCES=16        # Number of sequences per batch (batch size)</span><br><span class="line">UPDATE_FREQ=16          # Increase the batch size 16x</span><br><span class="line"></span><br><span class="line">DATA_DIR=data-bin/wikitext-<span class="number">103</span></span><br><span class="line"></span><br><span class="line">fairseq-train --fp16 $DATA_DIR \</span><br><span class="line">    --task masked_lm --criterion masked_lm \</span><br><span class="line">    --arch roberta_base --sample-<span class="keyword">break</span>-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \</span><br><span class="line">    --optimizer adam --adam-betas <span class="string">&#x27;(0.9,0.98)&#x27;</span> --adam-eps <span class="number">1e-6</span> --clip-norm <span class="number">0.0</span> \</span><br><span class="line">    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \</span><br><span class="line">    --dropout <span class="number">0.1</span> --attention-dropout <span class="number">0.1</span> --weight-decay <span class="number">0.01</span> \</span><br><span class="line">    --batch-size $MAX_SENTENCES --update-freq $UPDATE_FREQ \</span><br><span class="line">    --max-update $TOTAL_UPDATES --log-format simple --log-interval <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>如果出现如下错误信息：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Error</span>: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible <span class="keyword">with</span> libgomp.so<span class="number">.1</span> library.</span><br><span class="line">        Try to <span class="keyword">import</span> numpy first or set the threading layer accordingly. <span class="built_in">Set</span> MKL_SERVICE_FORCE_INTEL to force it.</span><br></pre></td></tr></table></figure>
<p>在*~/.bashrc* 中添加<code>export MKL_THREADING_LAYER=GNU</code>就可以解决：</p>
<p>Warning</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/home/User/miniconda3/lib/python3<span class="number">.8</span>/site-packages/torch/nn/parallel/distributed.py:<span class="number">397</span>: UserWarning: The <span class="string">`check_reduction`</span> argument <span class="keyword">in</span> <span class="string">`DistributedDataParallel`</span> <span class="built_in">module</span> is deprecated. Please avoid using it.</span><br></pre></td></tr></table></figure>






<p>$fairseq$模型解析</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/141210591">https://zhuanlan.zhihu.com/p/141210591</a></p>
<p>![截屏2020-12-28 下午4.38.11](/Users/cbl/MyBlog/source/_posts/截屏2020-12-28 下午4.38.11.png)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/12/28/RoBERTa%20Usage/" data-id="ckj2e8jgl0001jkfya8w3c51g" data-title="RoBERTa Usage" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2020/12/25/AttributeError-module-enum-has-no-attribute-IntFlag/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">AttributeError: module &#39;enum&#39; has no attribute &#39;IntFlag&#39;</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/12/28/RoBERTa%20Usage/">RoBERTa Usage</a>
          </li>
        
          <li>
            <a href="/2020/12/25/AttributeError-module-enum-has-no-attribute-IntFlag/">AttributeError: module &#39;enum&#39; has no attribute &#39;IntFlag&#39;</a>
          </li>
        
          <li>
            <a href="/2020/12/25/%E6%9F%A5%E7%9C%8B%E5%BA%93%E7%9A%84%E7%89%88%E6%9C%AC/">查看库的版本</a>
          </li>
        
          <li>
            <a href="/2020/12/24/Docker%20installation%20in%20Mac/">Docker installation</a>
          </li>
        
          <li>
            <a href="/2020/12/24/hexo-blog-in-Mac/">hexo-blog-in-Mac</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2020 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>